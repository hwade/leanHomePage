# 数据挖掘算法汇总

> 算法简介、应用及其优缺点

- [有监督学习](#supervise)

- [无监督学习](#unsupervise)

<h3 id="supervise">监督学习（分类算法）</h3>

```
根据数据样本上抽取出的特征，判定其属于有限类别中的哪一个
如：垃圾邮件识别（垃圾邮件|正常邮件）、文本情感分析（褒|贬）、图像识别（人|猫|狗|其他）
```
- [k-近邻（kNN）]()

存在一组已经分好类（标签）的样本数据（训练样本集），在输入没有分类（标签）的新数据后，将新数据的每一个特征与样本集中数据的对应特征进行比较，提取样本集中前k个最相似（最近邻）数据的分类标签作为新数据的备选分类标签，k由人为规定，一般不超过20，得出的备选标签中出现次数最多的作为新数据的分类标签。
	
比较 | KNN 
---- | -------------	
优点 | 精度高、对异常值不敏感、无数据输入假定
缺点 | 计算复杂度高、空间复杂度高
数据 | 数值型、标称(离散)型

- [决策树（Decision Tree）]()

	-	[ID3](), [C4.5](), [CART]()
	
在已知各种情况发生概率的基础上，根据特征值来划分样本类别（叶子节点为类别，中间节点为划分特征）。为了使所划分的类别最准确（也即每个类别内的信息凌乱程度最低，最接近同一类），需要选取能使集合无序程度最低（最大信息增益或者基尼系数最低）的特征进行每一次的划分，每划分一次就从特征序列中去除已划分的特征，直到所有已知类别（走到叶子节点）划分完或者特征列表中没有特征进行划分（此时将剩下的样本归为类别出现次数最多的那一类中）。在测试数据时将数据输入到决策树可以根据特征得到分类结果。

比较 | Decision Tree 
---- | -------------
优点 | （测试数据）计算复杂度不高、输出结果易于理解、中间值缺失不敏感、可处理不相关特征数据
缺点 | 可能会过度匹配（可减枝）、无法直接处理数值型数据（要量化为标称型）
数据 | 数值型、标称型

	
- [朴素贝叶斯(Naive Bayes)]()

基于贝叶斯决策理论，若样本属于类别一的概率_p1_和属于样本二的概率_p2_相比，_p1_ < _p2_则认为改样本属于类别二，可以使用公式来计算样本概率：

_p_(_Ci_ | _W0_,_W1_,_W2_..._Wn_) = _p_(_W0_,_W1_,_W2_..._Wn_ | _Ci_) \* _p_(_Ci_) / _p_(_W0_,_W1_,_W2_..._Wn_)

朴素贝叶斯是用统计学知识，并假设样本特征_Wi_之间是独立的，这样就可以简化条件概率公式：

_p_(_W0_,_W1_,_W2_..._Wn_) = _p_(_W0_) \* _p_(_W1_) \* _p_(_W2_) \* ... \* _p_(_Wn_)

_p_(_W0_,_W1_,_W2_..._Wn_ | _Ci_) = _p_(_W0_ | _Ci_) \* _p_(_W1_ | _Ci_) \* _p_(_W2_ | _Ci_) \* ... \* _p_(_Wn_ | _Ci_)

构建完分类器只要输入测试数据，比较不同类别的概率大小即可输出分类结果。

比较 |  Naive Bayes
---- | -------------
优点 | 数据较少的情况下仍然有效、可以处理多类别问题
缺点 | 对于输入数据的准备方式较为敏感（需要对特征值初始化为有效值，否则会出现很大偏差，即乘积结果为0，还需做自然对数处理）
数据 | 标称型
